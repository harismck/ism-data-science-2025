{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA, Clustering and Embeddings\n",
    "\n",
    "Today we'll analyse the BBC News Articles dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remember to run pip install sentence-transformers\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://huggingface.co/datasets/SetFit/bbc-news/resolve/main/bbc-text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "sport            511\n",
       "business         510\n",
       "politics         417\n",
       "tech             401\n",
       "entertainment    386\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vera drake leads uk oscar hopes mike leigh s film vera drake will lead british hopes at this year s academy awards after getting three nominations.  imelda staunton was nominated for best actress for her role in the abortion drama  while leigh received nods for best director and original screenplay. kate winslet was also nominated in the best actress category for her role in eternal sunshine of the spotless mind. and clive owen and sophie okonedo both got nominated for supporting roles in closer and hotel rwanda respectively. owen has already been made bookmakers  favourite for best supporting actor for the role in closer that has already clinched him a golden globe award.  and it is the first nomination for actress okonedo  chosen for her performance in hotel rwanda  about the 1994 rwandan genocide. it is also a debut nomination for staunton  49  who told bbc news 24 she had not thought the film would appeal to academy voters.  it was an extraordinary time making the film and i can t believe what has happened this morning   she said.  i hope it just shows mike up to be the extraordinary filmmaker he is.  we are also dealing with a very difficult subject matter and it is amazing to have it accepted in this way.  leigh  who had previously received three oscar nominations for secrets and lies and topsy turvy  told bbc news 24 the latest success was  amazing . he said:  we hoped that imelda staunton would get a nomination but i never expected to get director and screenplay. it s just absolutely wonderful.   i think people are aware that it s about life - and i hope it is the warmth and compassion that really talks to people.  winslet said she was  ecstatic  about the fourth nomination of her career.  being nominated means so much to me. to be nominated for a film that was released a while ago  i feel so honoured and overwhelmed   she said. john woodward  chief executive of the uk film council  said it was  extremely heartening  to see british filmmaking talent recognised on the global stage.  britain has a hugely talented industry and these nominations show why national lottery investment in film pays major dividends for our culture and economy.  among a total of 24 british nominees  composer andrew lloyd webber and lyricist charles hart are up for best original song for learn to be lonely  from the phantom of the opera movie.  cinematographer john mathieson  who was nominated for gladiator in 2001  is also up for the phantom of the opera. and finding neverland has garnered two more nominations for brits. gemma jackson  who has also worked on bridget jones s diary and iris  is up for art direction while costume designer alexandra byrne  whose previous films have included captain corelli s mandolin and elizabeth  is in the running. the uk has two contenders in the best live action short film category. wasp was made by ex-children s tv presenter andrea arnold while little terrorist is the work of ashvin kumar. this year s awards will be handed out in hollywood on 27 february.'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample()[\"text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ebb454a29d480eb8072d7ea2637dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "emb_df = pd.DataFrame(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001554</td>\n",
       "      <td>-0.067275</td>\n",
       "      <td>0.011174</td>\n",
       "      <td>-0.097146</td>\n",
       "      <td>0.054098</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>-0.034863</td>\n",
       "      <td>-0.017126</td>\n",
       "      <td>0.062448</td>\n",
       "      <td>-0.024317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083276</td>\n",
       "      <td>-0.012365</td>\n",
       "      <td>0.099596</td>\n",
       "      <td>-0.002854</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>0.069352</td>\n",
       "      <td>-0.008165</td>\n",
       "      <td>-0.015212</td>\n",
       "      <td>-0.063724</td>\n",
       "      <td>0.084557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.083547</td>\n",
       "      <td>0.059484</td>\n",
       "      <td>-0.013196</td>\n",
       "      <td>-0.011908</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.117316</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.013166</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056850</td>\n",
       "      <td>-0.037205</td>\n",
       "      <td>0.039960</td>\n",
       "      <td>0.014939</td>\n",
       "      <td>-0.075912</td>\n",
       "      <td>-0.010551</td>\n",
       "      <td>0.011098</td>\n",
       "      <td>-0.093159</td>\n",
       "      <td>-0.002457</td>\n",
       "      <td>0.021243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.055962</td>\n",
       "      <td>-0.008481</td>\n",
       "      <td>-0.025843</td>\n",
       "      <td>-0.056737</td>\n",
       "      <td>-0.045265</td>\n",
       "      <td>-0.021684</td>\n",
       "      <td>0.030081</td>\n",
       "      <td>-0.013983</td>\n",
       "      <td>0.030562</td>\n",
       "      <td>-0.003697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>-0.006896</td>\n",
       "      <td>-0.006589</td>\n",
       "      <td>-0.018533</td>\n",
       "      <td>-0.056650</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>-0.056027</td>\n",
       "      <td>0.043895</td>\n",
       "      <td>-0.046996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015004</td>\n",
       "      <td>-0.125696</td>\n",
       "      <td>-0.028034</td>\n",
       "      <td>-0.040649</td>\n",
       "      <td>0.080588</td>\n",
       "      <td>0.052048</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>-0.012288</td>\n",
       "      <td>0.033973</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>-0.004015</td>\n",
       "      <td>-0.057729</td>\n",
       "      <td>-0.021071</td>\n",
       "      <td>0.009793</td>\n",
       "      <td>0.056244</td>\n",
       "      <td>-0.032304</td>\n",
       "      <td>-0.020664</td>\n",
       "      <td>-0.025317</td>\n",
       "      <td>0.056185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.018273</td>\n",
       "      <td>-0.018895</td>\n",
       "      <td>-0.047966</td>\n",
       "      <td>-0.070612</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>0.059953</td>\n",
       "      <td>-0.119612</td>\n",
       "      <td>0.026853</td>\n",
       "      <td>0.050197</td>\n",
       "      <td>-0.011960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051296</td>\n",
       "      <td>0.027649</td>\n",
       "      <td>-0.022006</td>\n",
       "      <td>-0.028721</td>\n",
       "      <td>-0.003901</td>\n",
       "      <td>0.082542</td>\n",
       "      <td>-0.065968</td>\n",
       "      <td>-0.073110</td>\n",
       "      <td>-0.035203</td>\n",
       "      <td>0.043847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>-0.027352</td>\n",
       "      <td>-0.026011</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.025434</td>\n",
       "      <td>-0.014118</td>\n",
       "      <td>0.073263</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>0.068582</td>\n",
       "      <td>-0.033815</td>\n",
       "      <td>-0.002269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>-0.027655</td>\n",
       "      <td>-0.055435</td>\n",
       "      <td>-0.022257</td>\n",
       "      <td>-0.029578</td>\n",
       "      <td>-0.062166</td>\n",
       "      <td>-0.029384</td>\n",
       "      <td>-0.129902</td>\n",
       "      <td>-0.041675</td>\n",
       "      <td>0.080504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>-0.017127</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.017573</td>\n",
       "      <td>-0.024224</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>0.034838</td>\n",
       "      <td>0.082775</td>\n",
       "      <td>-0.099431</td>\n",
       "      <td>-0.101438</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045735</td>\n",
       "      <td>-0.004506</td>\n",
       "      <td>-0.019301</td>\n",
       "      <td>-0.011101</td>\n",
       "      <td>-0.026871</td>\n",
       "      <td>0.096571</td>\n",
       "      <td>-0.026657</td>\n",
       "      <td>0.012036</td>\n",
       "      <td>-0.049906</td>\n",
       "      <td>0.012583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>0.027212</td>\n",
       "      <td>-0.136341</td>\n",
       "      <td>0.026965</td>\n",
       "      <td>-0.063657</td>\n",
       "      <td>-0.002635</td>\n",
       "      <td>0.035392</td>\n",
       "      <td>0.036003</td>\n",
       "      <td>-0.031810</td>\n",
       "      <td>0.021682</td>\n",
       "      <td>0.062556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.031709</td>\n",
       "      <td>-0.021834</td>\n",
       "      <td>0.052545</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.055665</td>\n",
       "      <td>-0.050568</td>\n",
       "      <td>-0.092031</td>\n",
       "      <td>-0.077371</td>\n",
       "      <td>-0.007813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>0.046371</td>\n",
       "      <td>-0.035961</td>\n",
       "      <td>0.065271</td>\n",
       "      <td>-0.032036</td>\n",
       "      <td>0.030133</td>\n",
       "      <td>0.061473</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>0.036582</td>\n",
       "      <td>0.081157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069589</td>\n",
       "      <td>0.028942</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>-0.033583</td>\n",
       "      <td>-0.061675</td>\n",
       "      <td>0.092261</td>\n",
       "      <td>0.012805</td>\n",
       "      <td>0.045767</td>\n",
       "      <td>-0.016248</td>\n",
       "      <td>0.048731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>-0.021064</td>\n",
       "      <td>0.087468</td>\n",
       "      <td>-0.068830</td>\n",
       "      <td>-0.031641</td>\n",
       "      <td>0.084106</td>\n",
       "      <td>0.084725</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>0.087274</td>\n",
       "      <td>0.061285</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041510</td>\n",
       "      <td>-0.043472</td>\n",
       "      <td>-0.080778</td>\n",
       "      <td>-0.023821</td>\n",
       "      <td>-0.068595</td>\n",
       "      <td>0.017042</td>\n",
       "      <td>-0.008418</td>\n",
       "      <td>-0.063332</td>\n",
       "      <td>-0.090867</td>\n",
       "      <td>0.051220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.001554 -0.067275  0.011174 -0.097146  0.054098  0.042254 -0.034863   \n",
       "1    -0.083547  0.059484 -0.013196 -0.011908  0.011631  0.002616  0.117316   \n",
       "2    -0.055962 -0.008481 -0.025843 -0.056737 -0.045265 -0.021684  0.030081   \n",
       "3     0.015004 -0.125696 -0.028034 -0.040649  0.080588  0.052048  0.025743   \n",
       "4    -0.018273 -0.018895 -0.047966 -0.070612 -0.006405  0.059953 -0.119612   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2220 -0.027352 -0.026011  0.054015  0.025434 -0.014118  0.073263 -0.022070   \n",
       "2221 -0.017127  0.009932  0.017573 -0.024224  0.050684  0.034838  0.082775   \n",
       "2222  0.027212 -0.136341  0.026965 -0.063657 -0.002635  0.035392  0.036003   \n",
       "2223  0.046371 -0.035961  0.065271 -0.032036  0.030133  0.061473 -0.000475   \n",
       "2224 -0.021064  0.087468 -0.068830 -0.031641  0.084106  0.084725  0.031946   \n",
       "\n",
       "           7         8         9    ...       374       375       376  \\\n",
       "0    -0.017126  0.062448 -0.024317  ...  0.083276 -0.012365  0.099596   \n",
       "1     0.002310  0.013166  0.028018  ...  0.056850 -0.037205  0.039960   \n",
       "2    -0.013983  0.030562 -0.003697  ...  0.014019 -0.006896 -0.006589   \n",
       "3    -0.012288  0.033973  0.002043  ...  0.004305 -0.004015 -0.057729   \n",
       "4     0.026853  0.050197 -0.011960  ...  0.051296  0.027649 -0.022006   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2220  0.068582 -0.033815 -0.002269  ... -0.010152 -0.027655 -0.055435   \n",
       "2221 -0.099431 -0.101438  0.020063  ...  0.045735 -0.004506 -0.019301   \n",
       "2222 -0.031810  0.021682  0.062556  ...  0.032426  0.031709 -0.021834   \n",
       "2223 -0.001458  0.036582  0.081157  ...  0.069589  0.028942  0.002779   \n",
       "2224  0.087274  0.061285  0.004968  ...  0.041510 -0.043472 -0.080778   \n",
       "\n",
       "           377       378       379       380       381       382       383  \n",
       "0    -0.002854  0.030708  0.069352 -0.008165 -0.015212 -0.063724  0.084557  \n",
       "1     0.014939 -0.075912 -0.010551  0.011098 -0.093159 -0.002457  0.021243  \n",
       "2    -0.018533 -0.056650 -0.000594  0.039833 -0.056027  0.043895 -0.046996  \n",
       "3    -0.021071  0.009793  0.056244 -0.032304 -0.020664 -0.025317  0.056185  \n",
       "4    -0.028721 -0.003901  0.082542 -0.065968 -0.073110 -0.035203  0.043847  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2220 -0.022257 -0.029578 -0.062166 -0.029384 -0.129902 -0.041675  0.080504  \n",
       "2221 -0.011101 -0.026871  0.096571 -0.026657  0.012036 -0.049906  0.012583  \n",
       "2222  0.052545  0.000720  0.055665 -0.050568 -0.092031 -0.077371 -0.007813  \n",
       "2223 -0.033583 -0.061675  0.092261  0.012805  0.045767 -0.016248  0.048731  \n",
       "2224 -0.023821 -0.068595  0.017042 -0.008418 -0.063332 -0.090867  0.051220  \n",
       "\n",
       "[2225 rows x 384 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If above cell takes too long to run, use the precomputed embeddings\n",
    "emb_df = pd.read_csv(\"https://raw.githubusercontent.com/harismck/ism-data-science-2025/master/week9/bbc_embeddings.csv\", index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 384)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "- Reduce the dimensions of the embeddings dataset to two principal components. \n",
    "- Assign the principal component values to the original dataframe `df`.\n",
    "- Plot the result on a scatterplot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Print out several articles that are close to one another on the scatterplot above. Is their content similar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "- Cluster the articles into three clusters using k-means clustering.\n",
    "- Visualize the clusters on a scatterplot.\n",
    "- Do the clusters have something in common in terms of article category?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "Find the optimal number of clusters bit fitting kmeans multiple times for increasing number of `n_clusters`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "Fit kmeans with another value for n_clusters and visualize the clusters. Show which article categories belong to the clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "Train a model (RandomForest or LogisticRegression) for predicting whether an article belongs to the politics category. The model `pipeline` should take the full embedding as input and perform PCA before feeding it into the next steps of the pipeline. You should also perform hyperparameter tuning for at least one hyperparameter.\n",
    "\n",
    "As the last step, report the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "\n",
    "Calculate feature importances of the principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
