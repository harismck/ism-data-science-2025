{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks 2\n",
    "\n",
    "Today we'll talk about backpropagation and more complex neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/intro-stat-learning/ISLP/main/ISLP/data/Credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Income\", \"Limit\", \"Balance\"]]\n",
    "df = df.rename(columns=str.lower)\n",
    "df = (df - df.mean()) / df.std()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where we ended last time\n",
    "\n",
    "We finished our last lecture with this neural network:\n",
    "\n",
    "<img src=\"one_feature_one_neuron.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "The loss of this network is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}\\bigl(w^{(1)},b^{(1)},w^{(2)},b^{(2)}\\bigr)\n",
    "  = \\sum_{i=1}^{m}\n",
    "    \\left(\n",
    "      w^{(2)}\\,\\sigma\\!\\bigl(w^{(1)}x_i + b^{(1)}\\bigr)\n",
    "      + b^{(2)}\n",
    "      - y_i\n",
    "    \\right)^{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[\"limit\"].values\n",
    "y = df[\"balance\"].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# loss\n",
    "def sum_of_squares(a, y):\n",
    "    return np.sum((a - y) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.float32(1)\n",
    "b1 = np.float32(1)\n",
    "w2 = np.float32(1)\n",
    "b2 = np.float32(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1444.3526409019069)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = X * w1 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1 * w2 + b2\n",
    "L = sum_of_squares(z2, y)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to perform gradient descent, for which we have to find these four gradients:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w^{(2)}};\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b^{(2)}};\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w^{(1)}};\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b^{(1)}}\n",
    "$$\n",
    "\n",
    "\n",
    "The problem is that network now contains functions of functions - $z^{(2)}$ is a function of $a^{(1)}$, $w^{(2)}$ and $b^{(2)}$, and $a^{(1)}$ is itself a function of $w^{(1)}$ and $b^{(1)}$. When we have to find partial derivatives of a composite function $f(g(x))$, we turn to the chain rule.\n",
    "\n",
    "Given the derivative of the sigmoid activation function:\n",
    "\n",
    "$$\n",
    "\n",
    "(a^{(1)}_i)' = a^{(1)}_i * (1 - a^{(1)}_i)\n",
    "\\\\\n",
    "\n",
    "$$\n",
    "\n",
    "The formulas for the gradients are:\n",
    "\n",
    "$$\n",
    "\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w^{(2)}} = 2\\sum_{i=1}^{m}\\!\\Bigl( w^{(2)}a^{(1)}  + b^{(2)} - y_i \\Bigr)\\,a^{(1)}\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b^{(2)}} = 2\\sum_{i=1}^{m}\\!\\Bigl( w^{(2)}a^{(1)}  + b^{(2)} - y_i \\Bigr)\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial w^{(1)}} = 2\\sum_{i=1}^{m}\\!\\Bigl( w^{(2)}a^{(1)}  + b^{(2)} - y_i \\Bigr)\\,w^{(2)}\\,(a^{(1)})'x_i\n",
    "\\\\\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial b^{(1)}} = 2\\sum_{i=1}^{m}\\!\\Bigl( w^{(2)}a^{(1)}  + b^{(2)} - y_i \\Bigr)\\,w^{(2)}\\,(a^{(1)})'\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "We can actually ignore the 2 in the front in the formulas above, since it just scales the gradients, and we will scale them anyways using the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Finish the code for the backward pass below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w1 = np.float32(2)\n",
    "b1 = np.float32(2)\n",
    "w2 = np.float32(2)\n",
    "b2 = np.float32(2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# forward pass\n",
    "z1 = X * w1 + b1\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1 * w2 + b2\n",
    "\n",
    "# loss\n",
    "L = sum_of_squares(z2, y)\n",
    "\n",
    "# backward pass (backpropagation)\n",
    "err = z2 - y\n",
    "grad_w2 = ...\n",
    "grad_b2 = ...\n",
    "grad_w1 = ...\n",
    "grad_b1 = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Complete the code below using the code you wrote for the backward pass. Also implement the gradient descent part.\n",
    "\n",
    "Print out the weights and visualize the function that your model is fitting to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "w1 = np.float32(2)\n",
    "b1 = np.float32(2)\n",
    "w2 = np.float32(2)\n",
    "b2 = np.float32(2)\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "for i in range(100000):\n",
    "    # forward pass\n",
    "    z1 = X * w1 + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = a1 * w2 + b2\n",
    "\n",
    "    # loss\n",
    "    L = sum_of_squares(z2, y)\n",
    "\n",
    "    # backprop\n",
    "\n",
    "    # gradient descent\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More neurons\n",
    "\n",
    "Let's stick with one feature, but add more neurons to that hidden layer. The complication now is that we have more weights, and even though an individual weight is still scalar, writing backprop for every individual weight will quickly become unmanageable and inefficient. Therefore, from now on we will perform matrix operations.\n",
    "\n",
    "<img src=\"one_feature_two_neurons.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"limit\"]].values\n",
    "y = df[[\"balance\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement the forward pass of the illustrated network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.],\n",
       "       [3.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relevant numpy operations\n",
    "X1 = np.ones((3, 3))\n",
    "X2 = np.ones((3, 1))\n",
    "X1 @ X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement a single step of gradient descent for the neural network. Since we are no longer working with scalars, our gradient calculations became a little more complex as well. The exact formulas are given below.\n",
    "\n",
    "Run the cell that performs gradient descent several times, while printing out the weights. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement a function `gradient_descent` that performs gradient descent of the neural network. Use it to perform gradient descent, then print out the weights. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W1, b1, W2, b2, lr=0.0001, n_iter=10000):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Fix the problem above by choosing weights randomly. Plot the resulting fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.81777698,  1.1174691 ,  0.87424499,  0.04300084, -0.0602238 ],\n",
       "       [ 0.72956651, -0.71185215,  0.95446663, -0.06446925, -0.20007131],\n",
       "       [-1.41299271,  0.05206524,  0.05042646,  0.12670112, -0.69544944],\n",
       "       [-0.4858928 , -1.237632  , -0.25682642, -0.10415388, -1.86087575],\n",
       "       [ 1.07840139,  0.37102853,  0.41772486, -1.77804709,  0.85468556]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement a network with 5 neurons in the hidden layer, and run gradient descent for 20k iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two input features, single neuron\n",
    "\n",
    "\n",
    "<img src=\"two_features_one_neuron.png\" width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement the forward pass of the neural networks above. Then, perform gradient descent and print the weights. Use random initialization for the weights. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two features, two neurons\n",
    "\n",
    "Now let's look at the most complex neural network we will fit today.\n",
    "\n",
    "<img src=\"two_features_two_neurons.png\" width=\"1000\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Implement the forward pass of the network shown above. Train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "So far we worked with neural networks that are suited for regression tasks. Write a forward pass of a neural network that is suited for classification.\n",
    "\n",
    "Also consider and answer the following - do we need to change the procedure for gradient descent? If so, in what ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Congrats, you just implemented and trained a bunch of neural networks!\n",
    "\n",
    "Here are the key components and concepts to remember:\n",
    "- A feedforward neural network is essentially a series of matrix transformations and nonlinear transformations stacked together.\n",
    "- Even though we only looked at networks with one hidden layer, we can have as many hidden layers as we like (or as many as your machine allows). Same goes for neurons within each layer.\n",
    "- Nonlinear activation functions are what allow the neural network to fit complex functions to the data. Sigmoid is one such function, but there are many more.\n",
    "- The trickiest part (at least conceptually) about deep neural networks is training them, since we have to figure out partial derivatives of the loss function with respect to every parameter, which requires the application of the chain rule.\n",
    "- We can use these derivatives to calculate the  of the loss function with respect to each weight, via the process called backpropagation.\n",
    "- The process of adjusting weights according to these gradients is called gradient descent - we move weights in the direction that reduces the loss the fastest.\n",
    "- Learning rate controls the speed of gradient descent.\n",
    "\n",
    "You do not need to remember the formulas of the derivatives or how to derive them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
